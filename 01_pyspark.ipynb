{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D-Vaibhav/google_colab/blob/pyspark/01_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "pmQVjlpcJlOb",
        "outputId": "128854d9-4821-4de4-d068-5be15a34c3c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3f037eb19691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pyspark' is not defined"
          ]
        }
      ],
      "source": [
        "print(pyspark.__version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kepS1pNcJ17P",
        "outputId": "71b50af7-128c-4927-f5a4-62b30585f59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark\n",
        "\n",
        "https://medium.com/walmartglobaltech/decoding-memory-in-spark-parameters-that-are-often-confused-c11be7488a24\n",
        "\n",
        "https://sparkbyexamples.com/spark/spark-show-display-dataframe-contents-in-table/\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-count/\n",
        "\n",
        "https://sparkbyexamples.com/pyspark/pyspark-where-filter/"
      ],
      "metadata": {
        "id": "Mk_qI6ZezYc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "ls -l ./sample_data/*.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mawZ-TVD3OGO",
        "outputId": "0208c873-2744-424f-bf1e-dedc0940d600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root   301141 Mar 20 13:36 ./sample_data/california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Mar 20 13:36 ./sample_data/california_housing_train.csv\n",
            "-rw-r--r-- 1 root root 18289443 Mar 20 13:36 ./sample_data/mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Mar 20 13:36 ./sample_data/mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGyQJP84MGGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69d2677-bc86-4ba3-9c93-c9840837eb1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=quick_revision>\n",
            "root\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- housing_median_age: string (nullable = true)\n",
            " |-- total_rooms: string (nullable = true)\n",
            " |-- total_bedrooms: string (nullable = true)\n",
            " |-- population: string (nullable = true)\n",
            " |-- households: string (nullable = true)\n",
            " |-- median_income: string (nullable = true)\n",
            " |-- median_house_value: string (nullable = true)\n",
            "\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000| 11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000|237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000|204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000|218.000000|     1.664100|      67000.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 7 rows\n",
            "\n",
            "calling function => basic_operation_via_df_api --------------------------\n",
            "total_rows are 3000\n",
            "\n",
            "\n",
            "DataFrame[longitude: string, latitude: string]\n",
            "+-----------+---------+\n",
            "|  longitude| latitude|\n",
            "+-----------+---------+\n",
            "|-122.050000|37.370000|\n",
            "|-118.300000|34.260000|\n",
            "|-117.810000|33.780000|\n",
            "|-118.360000|33.820000|\n",
            "|-119.670000|36.330000|\n",
            "|-119.560000|36.510000|\n",
            "|-121.430000|38.630000|\n",
            "+-----------+---------+\n",
            "only showing top 7 rows\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DataFrame[longitude: string, latitude: string]\n",
            "+-----------+---------+\n",
            "|  longitude| latitude|\n",
            "+-----------+---------+\n",
            "|-118.070000|33.930000|\n",
            "|-121.800000|37.990000|\n",
            "|-118.270000|34.110000|\n",
            "|-118.180000|34.020000|\n",
            "|-118.600000|34.130000|\n",
            "|-118.440000|34.170000|\n",
            "|-120.560000|35.480000|\n",
            "+-----------+---------+\n",
            "only showing top 7 rows\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DataFrame[longitude: string, latitude: string]\n",
            "+-----------+---------+\n",
            "|  longitude| latitude|\n",
            "+-----------+---------+\n",
            "|-122.050000|37.370000|\n",
            "|-118.300000|34.260000|\n",
            "|-117.810000|33.780000|\n",
            "|-118.360000|33.820000|\n",
            "|-119.670000|36.330000|\n",
            "|-119.560000|36.510000|\n",
            "|-121.430000|38.630000|\n",
            "+-----------+---------+\n",
            "only showing top 7 rows\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "DataFrame[longitude: string, latitude: string, long: double, lati: double]\n",
            "+-----------+---------+------+----+\n",
            "|  longitude| latitude|  long|lati|\n",
            "+-----------+---------+------+----+\n",
            "|-122.050000|37.370000|-122.0|37.0|\n",
            "|-118.300000|34.260000|-118.0|34.0|\n",
            "|-117.810000|33.780000|-118.0|34.0|\n",
            "|-118.360000|33.820000|-118.0|34.0|\n",
            "|-119.670000|36.330000|-120.0|36.0|\n",
            "|-119.560000|36.510000|-120.0|37.0|\n",
            "|-121.430000|38.630000|-121.0|39.0|\n",
            "+-----------+---------+------+----+\n",
            "only showing top 7 rows\n",
            "\n",
            "df1.count() is 3000 \n",
            "\n",
            "\n",
            "DataFrame[long: double, count(lati): bigint]\n",
            "+------+-----------+\n",
            "|  long|count(lati)|\n",
            "+------+-----------+\n",
            "|-114.0|          1|\n",
            "|-115.0|          5|\n",
            "|-116.0|         25|\n",
            "|-117.0|        372|\n",
            "|-118.0|       1098|\n",
            "|-119.0|        182|\n",
            "|-120.0|        184|\n",
            "+------+-----------+\n",
            "only showing top 7 rows\n",
            "\n",
            "df10.count() is 11 \n",
            "\n",
            "\n",
            "DataFrame[sum(lati): bigint]\n",
            "+---------+\n",
            "|sum(lati)|\n",
            "+---------+\n",
            "|     3000|\n",
            "+---------+\n",
            "\n",
            "df_temp.count() is 1 \n",
            "\n",
            "\n",
            "+-----------+---------+------+----+\n",
            "|  longitude| latitude|  long|lati|\n",
            "+-----------+---------+------+----+\n",
            "|-122.050000|37.370000|-122.0|37.0|\n",
            "|-119.560000|36.510000|-120.0|37.0|\n",
            "|-121.930000|37.250000|-122.0|37.0|\n",
            "|-121.910000|37.440000|-122.0|37.0|\n",
            "|-121.930000|37.330000|-122.0|37.0|\n",
            "|-121.820000|37.250000|-122.0|37.0|\n",
            "|-121.780000|37.230000|-122.0|37.0|\n",
            "+-----------+---------+------+----+\n",
            "only showing top 7 rows\n",
            "\n",
            "+-----------+---------+------+----+\n",
            "|  longitude| latitude|  long|lati|\n",
            "+-----------+---------+------+----+\n",
            "|-122.050000|37.370000|-122.0|37.0|\n",
            "|-122.050000|37.360000|-122.0|37.0|\n",
            "|-122.050000|37.050000|-122.0|37.0|\n",
            "|-122.050000|37.310000|-122.0|37.0|\n",
            "|-122.050000|37.380000|-122.0|37.0|\n",
            "+-----------+---------+------+----+\n",
            "\n",
            "calling function => basic_operation_via_spark_sql --------------------------\n",
            "DataFrame[longitude: string, latitude: string, housing_median_age: string, total_rooms: string, total_bedrooms: string, population: string, households: string, median_income: string, median_house_value: string]\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000| 11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000|237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000|204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000|218.000000|     1.664100|      67000.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 7 rows\n",
            "\n",
            "df1.count() is 3000 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. IMPORTING PACKAGES ======================================================================================\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, round, count, desc, sum\n",
        "\n",
        "\n",
        "# 2. SETTING MAIN FUNCTION FOR OS CALLING ====================================================================\n",
        "def init_spark():\n",
        "    spark = SparkSession.builder\\\n",
        "                        .master('local[*]')\\\n",
        "                        .appName(\"quick_revision\")\\\n",
        "                        .getOrCreate()\n",
        "                        \n",
        "    sc = spark.sparkContext\n",
        "    return spark, sc\n",
        "\n",
        "\n",
        "def main():\n",
        "    spark, sc = init_spark()\n",
        "    print(sc)\n",
        "\n",
        "    california_housing_test = read_csv_data(spark, sc)\n",
        "\n",
        "    print('calling function => basic_operation_via_df_api --------------------------')\n",
        "    basic_operation_via_df_api(california_housing_test)\n",
        "\n",
        "    print('calling function => basic_operation_via_spark_sql --------------------------')\n",
        "    basic_operation_via_spark_sql(spark, california_housing_test)\n",
        "\n",
        "\n",
        "\n",
        "# 3. AD-HOC FUNCTIONs =======================================================================================\n",
        "def read_csv_data(spark, sc):\n",
        "    data_path = './sample_data/california_housing_test.csv'\n",
        "    california_housing_test = spark.read.csv(data_path, header=True, escape=',')\n",
        "    california_housing_test.printSchema()\n",
        "    california_housing_test.show(7)\n",
        "\n",
        "    return california_housing_test\n",
        "\n",
        "\n",
        "def basic_operation_via_df_api(df):\n",
        "    total_rows = df.count()\n",
        "    print(f'total_rows are {total_rows}\\n\\n')\n",
        "\n",
        "    # selecting columns -------------------------------------------------------------------------------\n",
        "    df1 = df.select('longitude', 'latitude')\n",
        "    print(df1)\n",
        "    df1.show(7)\n",
        "    print('\\n\\n')\n",
        "\n",
        "    df2 = df.select(df.longitude, df.latitude).distinct()\n",
        "    print(df2)\n",
        "    df2.show(7)\n",
        "    print('\\n\\n')\n",
        "\n",
        "    df3 = df.select(col('longitude'), col('latitude'))\n",
        "    print(df3)\n",
        "    df3.show(7)\n",
        "    print('\\n\\n')\n",
        "\n",
        "    # round, aliasing columns -------------------------------------------------------------------------------\n",
        "    df1 = df1.select('*', round('longitude').alias('long'), round('latitude').alias('lati'))\n",
        "    print(df1)\n",
        "    df1.show(7)\n",
        "    print(f'df1.count() is {df1.count()} \\n\\n')\n",
        "\n",
        "    # groupBy, agg, orderBy function columns -------------------------------------------------------------------------------\n",
        "    df10 = df1.groupBy('long').agg(count('lati')).orderBy(desc('long'))\n",
        "    print(df10)\n",
        "    df10.show(7)\n",
        "    print(f'df10.count() is {df10.count()} \\n\\n')\n",
        "\n",
        "    # df_temp = df10.select('count(lati)'.alias('lati'))\n",
        "    df_temp = df10.select(col('count(lati)').alias('lati'))\n",
        "    df_temp = df_temp.agg(sum('lati'))\n",
        "    print(df_temp)\n",
        "    df_temp.show(7)\n",
        "    print(f'df_temp.count() is {df_temp.count()} \\n\\n')\n",
        "\n",
        "    # where/filtering columns -------------------------------------------------------------------------------\n",
        "    df1.filter(df1.lati == '37.0').show(7)\n",
        "\n",
        "    df1.where((df1.lati == '37.0') & (df1.longitude == '-122.050000')).show(7)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def basic_operation_via_spark_sql(spark, df):\n",
        "    # creating view in-order to apply spark sql -------------------------------------------------------\n",
        "    df.createOrReplaceTempView('df')\n",
        "\n",
        "    df1 = spark.sql('select * from df')\n",
        "    print(df1)\n",
        "    df1.show(7)\n",
        "    print(f'df1.count() is {df1.count()} \\n\\n')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X13rbCeZ2pj6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfIVEMfFrFM4wHOZ9iuE55",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}